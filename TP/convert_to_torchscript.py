"""
Script pour convertir un mod√®le PyTorch au format TorchScript (.pt/.jit)
"""

import torch
import torch.nn as nn
import os
import argparse
from networks_2100078 import *


class ModelWrapper(nn.Module):
    """Wrapper pour rendre les mod√®les compatibles avec TorchScript"""
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.model.eval()
        
    def forward(self, x):
        # Appeler le mod√®le directement - il g√®re d√©j√† le preprocessing en interne
        return self.model(x)


def convert_model_to_torchscript(model_path, output_path=None, input_size=None, trace=True):
    """
    Convertit un mod√®le PyTorch sauvegard√© au format TorchScript.
    
    Args:
        model_path (str): Chemin vers le fichier .pth du mod√®le
        output_path (str): Chemin de sortie pour le mod√®le TorchScript (optionnel)
        input_size (tuple): Taille de l'exemple d'entr√©e pour le tracing (optionnel, auto-d√©tect√© si None)
        trace (bool): Si True, utilise torch.jit.trace, sinon utilise torch.jit.script
    
    Returns:
        str: Chemin du fichier TorchScript g√©n√©r√©
    """
    
    # Charger le mod√®le
    print(f"Chargement du mod√®le depuis: {model_path}")
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    
    # V√©rifier si le checkpoint est d√©j√† un mod√®le
    if isinstance(checkpoint, nn.Module):
        model_type_name = type(checkpoint).__name__
        print(f"Le fichier contient directement un mod√®le de type: {model_type_name}")
        model = checkpoint
        model.eval()
        
        # D√©tecter automatiquement la taille d'entr√©e si non sp√©cifi√©e
        if input_size is None:
            if 'LSTM' in model_type_name:
                # Pour les LSTM: (batch, seq_len, height, width)
                # Sera transform√© en (batch, seq_len, height*width) par le mod√®le
                input_size = (1, 1, 8, 8)
                print(f"Auto-d√©tection: Mod√®le LSTM - Taille d'entr√©e: {input_size}")
            elif 'CNN' in model_type_name:
                # Pour les CNN: (batch, channels, height, width)
                input_size = (1, 1, 8, 8)
                print(f"Auto-d√©tection: Mod√®le CNN - Taille d'entr√©e: {input_size}")
            else:
                # Pour les MLP: (batch, features) o√π features = 64
                input_size = (1, 64)
                print(f"Auto-d√©tection: Mod√®le MLP - Taille d'entr√©e: {input_size}")
    else:
        # Extraire le mod√®le du checkpoint
        if isinstance(checkpoint, dict):
            if 'model_state_dict' in checkpoint:
                model_state = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                model_state = checkpoint['state_dict']
            else:
                model_state = checkpoint
        else:
            print("Format de checkpoint non reconnu")
            return None
        
        # Cr√©er une instance du mod√®le
        # Note: Vous devrez peut-√™tre adapter cela en fonction de votre configuration
        conf = {
            "board_size": 8,
            "path_save": "save_models",
            "earlyStopping": 10,
            "len_inpout_seq": 1,
            "dropout": 0.0
        }
        
        # Essayer de d√©tecter le type de mod√®le √† partir des cl√©s
        if 'conv1.weight' in model_state or any('conv' in k for k in model_state.keys()):
            print("D√©tection d'un mod√®le CNN")
            model = CNN_32(conf)
            if input_size is None:
                input_size = (1, 1, 8, 8)
        elif 'lstm' in str(model_state.keys()).lower():
            print("D√©tection d'un mod√®le LSTM")
            model = LSTMHiddenState_64(conf)
            if input_size is None:
                input_size = (1, 1, 64)
        else:
            print("D√©tection d'un mod√®le MLP")
            model = MLP(conf)
            if input_size is None:
                input_size = (1, 64)
        
        # Charger les poids
        try:
            model.load_state_dict(model_state)
        except Exception as e:
            print(f"Erreur lors du chargement des poids: {e}")
            print("Essai avec strict=False...")
            model.load_state_dict(model_state, strict=False)
        
        model.eval()
    
    # G√©n√©rer le chemin de sortie si non sp√©cifi√©
    if output_path is None:
        base_name = os.path.splitext(model_path)[0]
        output_path = f"{base_name}_torchscript.pt"
    
    print(f"Conversion du mod√®le en TorchScript...")
    print(f"Taille d'entr√©e utilis√©e: {input_size}")
    
    try:
        # Cr√©er un exemple d'entr√©e
        example_input = torch.randn(*input_size)
        
        # Mettre le mod√®le en mode √©valuation
        model.eval()
        
        # Forcer la suppression de tout √©tat cach√© pour les LSTM
        if 'LSTM' in type(model).__name__:
            # R√©initialiser les √©tats cach√©s des LSTM
            if hasattr(model, 'lstm'):
                model.lstm.flatten_parameters()
            # S'assurer que le mod√®le est bien en mode eval
            for module in model.modules():
                if isinstance(module, torch.nn.LSTM):
                    module.flatten_parameters()
        
        # Test que le mod√®le fonctionne
        with torch.no_grad():
            try:
                output = model(example_input)
                print(f"‚úì Test de forward pass r√©ussi - Sortie: {output.shape}")
            except Exception as e:
                print(f"‚ö† Erreur lors du test forward avec forme {input_size}: {e}")
                # Essayer avec diff√©rentes formes d'entr√©e
                if 'LSTM' in type(model).__name__:
                    # Essayer plusieurs formes possibles pour LSTM
                    # Les LSTM avec np.squeeze peuvent avoir besoin de formes sp√©cifiques
                    possible_shapes = [
                        (2, 1, 8, 8),    # (batch>1, seq_len, height, width)
                        (1, 2, 8, 8),    # (batch, seq_len>1, height, width)
                        (2, 8, 8),       # (batch>1, height, width)
                    ]
                    for shape in possible_shapes:
                        try:
                            print(f"Essai avec forme {shape}...")
                            example_input = torch.randn(*shape)
                            output = model(example_input)
                            print(f"‚úì Test de forward pass r√©ussi - Forme: {shape} - Sortie: {output.shape}")
                            input_size = shape
                            break
                        except Exception as shape_error:
                            continue
                else:
                    print(f"Le mod√®le ne peut pas √™tre test√©, mais le mod√®le sera sauvegard√©.")
        
        # Essayer TorchScript trace (peut √©chouer avec numpy)
        if trace:
            try:
                print("\nTentative de tracing TorchScript (peut √©chouer avec numpy)...")
                with torch.no_grad():
                    traced_model = torch.jit.trace(model, example_input, strict=False)
                traced_model.save(output_path)
                print(f"‚úì Mod√®le TorchScript (traced) sauvegard√© √†: {output_path}")
                
                # V√©rifier que le mod√®le peut √™tre recharg√©
                print("V√©rification du mod√®le TorchScript...")
                loaded_model = torch.jit.load(output_path)
                with torch.no_grad():
                    test_output = loaded_model(example_input)
                print(f"‚úì Mod√®le TorchScript v√©rifi√© - Sortie: {test_output.shape}")
                return output_path
            except Exception as trace_error:
                print(f"‚ö† Tracing TorchScript √©chou√©: {trace_error}")
                print(f"\nüìå Les mod√®les avec numpy ne peuvent pas √™tre convertis en TorchScript.")
                print(f"üìå Sauvegarde du mod√®le complet en format PyTorch standard...")
                
                # Mettre le mod√®le en CPU pour √©viter les probl√®mes de compatibilit√©
                model_cpu = model.cpu()
                model_cpu.eval()
                
                # Pour les LSTM, s'assurer que les param√®tres sont aplatis
                if 'LSTM' in type(model_cpu).__name__:
                    for module in model_cpu.modules():
                        if isinstance(module, torch.nn.LSTM):
                            module.flatten_parameters()
                
                # Sauvegarder le mod√®le complet (pas juste les poids)
                torch.save(model_cpu, output_path, _use_new_zipfile_serialization=True)
                print(f"‚úì Mod√®le complet sauvegard√© √†: {output_path}")
                
            
            # Mettre le mod√®le en CPU pour √©viter les probl√®mes de compatibilit√©
            model_cpu = model.cpu()
            model_cpu.eval()
            
            # Pour les LSTM, s'assurer que les param√®tres sont aplatis
            if 'LSTM' in type(model_cpu).__name__:
                for module in model_cpu.modules():
                    if isinstance(module, torch.nn.LSTM):
                        module.flatten_parameters()
            
            # Sauvegarder le mod√®le complet (pas juste les poids)
            torch.save(model_cpu, output_path, _use_new_zipfile_serialization=True)
            print(f"‚úì Mod√®le complet sauvegard√© √†: {output_path}")
            
            # V√©rifier le rechargement
            print("V√©rification du mod√®le sauvegard√©...")
            loaded_model = torch.load(output_path, map_location='cpu', weights_only=False)
            loaded_model.eval()
            
            # Test avec l'entr√©e sur CPU
            example_input_cpu = example_input.cpu() if example_input.is_cuda else example_input
            with torch.no_grad():
                original_output = model_cpu(example_input_cpu)
                test_output = loaded_model(example_input_cpu)
                
                # V√©rifier que les sorties sont identiques
                if torch.allclose(original_output, test_output, rtol=1e-5, atol=1e-5):
                    print(f"‚úì Mod√®le v√©rifi√© - Sorties identiques: {test_output.shape}")
                else:
                    print(f"‚ö† ATTENTION: Les sorties diff√®rent!")
                    print(f"  Original: min={original_output.min():.6f}, max={original_output.max():.6f}")
                    print(f"  Recharg√©: min={test_output.min():.6f}, max={test_output.max():.6f}")
                    diff = torch.abs(original_output - test_output).max()
                    print(f"  Diff√©rence max: {diff:.6f}")
            
                    # V√©rifier que les sorties sont identiques
                    if torch.allclose(original_output, test_output, rtol=1e-5, atol=1e-5):
                        print(f"‚úì Mod√®le v√©rifi√© - Sorties identiques: {test_output.shape}")
                    else:
                        print(f"‚ö† ATTENTION: Les sorties diff√®rent!")
                        print(f"  Original: min={original_output.min():.6f}, max={original_output.max():.6f}")
                        print(f"  Recharg√©: min={test_output.min():.6f}, max={test_output.max():.6f}")
                        diff = torch.abs(original_output - test_output).max()
                        print(f"  Diff√©rence max: {diff:.6f}")
                
                return output_path
        else:
            print("\nüìå Note: torch.jit.script n'est pas support√© pour ces mod√®les (contiennent du code numpy)")
            print(f"üìå Sauvegarde du mod√®le complet en format PyTorch standard...")
            # Sauvegarder le mod√®le complet (pas juste les poids)
            # Mettre le mod√®le en CPU
            model_cpu = model.cpu()
            model_cpu.eval()
            
            # Pour les LSTM, aplatir les param√®tres
            if 'LSTM' in type(model_cpu).__name__:
                for module in model_cpu.modules():
                    if isinstance(module, torch.nn.LSTM):
                        module.flatten_parameters()
            
            torch.save(model_cpu, output_path, _use_new_zipfile_serialization=True)
            print(f"‚úì Mod√®le complet sauvegard√© √†: {output_path}")
            
            # V√©rifier le rechargement
            print("V√©rification du mod√®le sauvegard√©...")
            loaded_model = torch.load(output_path, map_location='cpu', weights_only=False)
            loaded_model.eval()
            with torch.no_grad():
                test_output = loaded_model(example_input)
            print(f"‚úì Mod√®le v√©rifi√© - Sortie: {test_output.shape}")
            return output_path
        
        # V√©rifier que le mod√®le peut √™tre recharg√©
        print("V√©rification du mod√®le TorchScript...")
        loaded_model = torch.jit.load(output_path)
        print("‚úì Mod√®le TorchScript charg√© avec succ√®s!")
        
        return output_path
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de la conversion: {e}")
        print(f"\nüìå Note: Les mod√®les contenant du code numpy ne peuvent g√©n√©ralement pas √™tre convertis en TorchScript.")
        print(f"üìå Sauvegarde du mod√®le complet en format PyTorch standard...")
        # Sauvegarder le mod√®le complet (pas juste les poids)
        try:
            torch.save(model, output_path, _use_new_zipfile_serialization=True)
            print(f"‚úì Mod√®le complet sauvegard√© √†: {output_path}")
            return output_path
        except Exception as save_error:
            print(f"‚ùå Impossible de sauvegarder le mod√®le: {save_error}")
            return None


def convert_directory(directory_path, output_dir=None, input_size=None):
    """
    Convertit tous les mod√®les .pth d'un r√©pertoire en TorchScript.
    
    Args:
        directory_path (str): Chemin du r√©pertoire contenant les mod√®les
        output_dir (str): R√©pertoire de sortie (optionnel)
        input_size (tuple): Taille de l'exemple d'entr√©e pour le tracing (optionnel, auto-d√©tect√© si None)
    """
    
    if output_dir is None:
        output_dir = os.path.join(directory_path, "torchscript_models")
    
    os.makedirs(output_dir, exist_ok=True)
    
    converted_count = 0
    failed_count = 0
    
    # Parcourir tous les fichiers .pth du r√©pertoire
    for root, dirs, files in os.walk(directory_path):
        for file in files:
            if file.endswith('.pth'):
                model_path = os.path.join(root, file)
                output_name = os.path.splitext(file)[0] + "_torchscript.pt"
                output_path = os.path.join(output_dir, output_name)
                
                print(f"\n{'='*60}")
                result = convert_model_to_torchscript(model_path, output_path, input_size)
                
                if result:
                    converted_count += 1
                else:
                    failed_count += 1
    
    print(f"\n{'='*60}")
    print(f"Conversion termin√©e!")
    print(f"Mod√®les convertis avec succ√®s: {converted_count}")
    print(f"√âchecs: {failed_count}")


def main():
    parser = argparse.ArgumentParser(description='Convertir un mod√®le PyTorch au format TorchScript')
    parser.add_argument('--model', type=str, help='Chemin vers le fichier .pth du mod√®le')
    parser.add_argument('--directory', type=str, help='Chemin vers un r√©pertoire contenant des mod√®les')
    parser.add_argument('--output', type=str, help='Chemin de sortie pour le mod√®le TorchScript')
    parser.add_argument('--input-size', type=str, default=None, 
                        help='Taille de l\'entr√©e (format: batch,height,width ou batch,channels,height,width). Auto-d√©tect√© si non sp√©cifi√©.')
    parser.add_argument('--method', type=str, choices=['trace', 'script'], default='trace',
                        help='M√©thode de conversion (trace ou script)')
    
    args = parser.parse_args()
    
    # Parser la taille d'entr√©e
    if args.input_size:
        input_size = tuple(map(int, args.input_size.split(',')))
    else:
        input_size = None
    trace = (args.method == 'trace')
    
    if args.directory:
        # Convertir tous les mod√®les d'un r√©pertoire
        convert_directory(args.directory, args.output, input_size)
    elif args.model:
        # Convertir un seul mod√®le
        convert_model_to_torchscript(args.model, args.output, input_size, trace)
    else:
        print("Usage:")
        print("  Convertir un seul mod√®le:")
        print("    python convert_to_torchscript.py --model chemin/vers/model.pth")
        print("\n  Convertir un r√©pertoire:")
        print("    python convert_to_torchscript.py --directory chemin/vers/repertoire")
        print("\nExemples:")
        print("  python convert_to_torchscript.py --model save_models_MLP/best_model.pth")
        print("  python convert_to_torchscript.py --directory save_models_MLP --output torchscript_models")
        print("  python convert_to_torchscript.py --model model.pth --input-size 1,8,8 --method script")


if __name__ == "__main__":
    main()
