# Pr√©sentation Orale - Optimisation de Mod√®les IA pour Othello
**Dur√©e : 5 minutes**

---

## üéØ Plan de Pr√©sentation

### 1. Introduction (30 secondes)
- **Contexte** : D√©veloppement d'une IA pour jouer √† Othello
- **Objectif** : Optimiser diff√©rents types de r√©seaux de neurones
- **Approche** : Grid search syst√©matique sur MLP, LSTM et CNN

### 2. Mod√®les MLP - Multi-Layer Perceptron (1 minute)
**Mod√®le de base :**
- Architecture : [128]
- Param√®tres : 33 088
- Performance : **14.97% accuracy** sur dev
- Temps : 109 secondes

**Meilleur mod√®le optimis√© :**
- Architecture : **[512, 256]** avec ReLU + Dropout 0.2
- Param√®tres : 205 760
- Performance : **35.84% accuracy** sur dev
- Diff√©rence train-dev : 8.01% (bon √©quilibre)
- **Am√©lioration : +139% vs baseline**

### 3. Mod√®les LSTM (1 minute)
**Mod√®le de base :**
- Architecture : LSTM Output Sequence [256]
- Param√®tres : 346 176
- Performance : **22.07% accuracy** sur dev
- Temps : 403 secondes

**Meilleur mod√®le optimis√© :**
- Architecture : **LSTM Hidden State [256]** avec ReLU + Dropout 0.2
- Param√®tres : 362 560
- Performance : **36.73% accuracy** sur dev
- Diff√©rence train-dev : 12.89% (overfitting mod√©r√©)
- **Am√©lioration : +66% vs baseline**

**Points cl√©s :**
- Hidden State > Output Sequence
- Batch size optimal : 500
- Optimizer : Adam (LR = 0.005)

### 4. Mod√®les CNN - Convolutional Neural Networks (1 minute 30 secondes)
**Architecture test√©e :** Kernel 3x3, Padding 1 (pr√©serve 8x8)

**Deux meilleurs mod√®les :**

**CNN [64, 128, 256] + ReLU :**
- Param√®tres : 1 418 304
- Performance : **43.87% accuracy** sur dev (5 epochs)
- Diff√©rence train-dev : 31.72%
- Temps : 389 secondes

**CNN [32, 64, 128] + ReLU :**
- Param√®tres : 617 024
- Performance : **44.15% accuracy** sur dev (10 epochs)
- Diff√©rence train-dev : 37.75%
- Temps : 1081 secondes
- **‚ö†Ô∏è Forte overfitting mais meilleures performances**

**Observation critique :** Les CNN montrent d'excellentes performances sur dev mais souffrent d'overfitting s√©v√®re ‚Üí N√©cessit√© d'augmentation des donn√©es

### 5. Comparaison Finale et Conclusion (1 minute)

#### Tableau R√©capitulatif des Meilleurs Mod√®les

| Mod√®le | Architecture | Params | Acc Dev | Diff Train-Dev | Temps |
|--------|-------------|---------|---------|----------------|-------|
| **MLP** | [512, 256] + ReLU | 205K | 35.84% | 8.01% | 829s |
| **LSTM** | HiddenState [256] + ReLU | 362K | 36.73% | 12.89% | 530s |
| **CNN** | [32, 64, 128] + ReLU | 617K | **44.15%** | **37.75%** | 1081s |

#### üìä Graphique de Comparaison - Accuracy Dev
```
CNN [32,64,128]     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 44.15%
CNN [64,128,256]    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 43.87%
LSTM Hidden [256]   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 36.73%
MLP [512,256]       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 35.84%
LSTM Base           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 22.07%
MLP Base            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 14.97%
```

#### üéØ Conclusions Principales

**Performances :**
- **CNN >> LSTM > MLP** en termes d'accuracy
- Les CNN capturent mieux la structure spatiale du plateau
- Am√©lioration totale : **+195%** (MLP base ‚Üí meilleur CNN)

**Probl√©matiques identifi√©es :**
1. **Overfitting s√©v√®re sur CNN** (diff√©rence train-dev de 38%)
2. Les performances en parties r√©elles ne correspondent pas toujours aux m√©triques
3. Dataset trop petit pour mod√®les complexes

**Solutions envisag√©es :**
- ‚úÖ Augmentation des donn√©es (en cours)
- ‚úÖ R√©gularisation (dropout, batch norm)
- ‚è≥ G√©n√©ration de nouvelles parties

**Choix du mod√®le final :**
- **Pour la production : MLP [512, 256]** (meilleur √©quilibre performance/g√©n√©ralisation)
- **Pour l'exploration : CNN [32, 64, 128]** (meilleures performances brutes)

---

## üìà Images de R√©f√©rence pour la Pr√©sentation

### Courbes du Meilleur MLP
![MLP Optimis√©](results/plots/learning_curve_MLP_512_256_Dropout_Relu_Post_Optimisation_20260108_212338.png)
- ‚úÖ Bon √©quilibre train/dev
- ‚úÖ Convergence stable

### Courbes du Meilleur LSTM
![LSTM Optimis√©](results/plots/learning_curve_LSTMHiddenState_Dropout_Relu_256_Post_Optimisation_20260108_212121.png)
- ‚ö†Ô∏è Overfitting mod√©r√©
- ‚úÖ Bonnes performances

### Courbes du Meilleur CNN
![CNN Optimis√©](results/plots/learning_curve_CNN_32_64_128_Dropout_Gridsearch_Relu_20260109_045319.png)
- ‚ùå Overfitting s√©v√®re (loss dev cro√Æt)
- ‚úÖ Meilleures performances absolues

---

## üí° Points Cl√©s √† Mentionner

1. **M√©thodologie rigoureuse** : Grid search syst√©matique sur tous les hyperparam√®tres
2. **Progression claire** : MLP ‚Üí LSTM ‚Üí CNN avec am√©lioration continue
3. **Trade-off identifi√©** : Performance brute vs capacit√© de g√©n√©ralisation
4. **Approche scientifique** : Identification des probl√®mes (overfitting) et solutions propos√©es

---

## ‚è±Ô∏è Timing Sugg√©r√©

| Section | Temps | Cumul |
|---------|-------|-------|
| Introduction | 30s | 0:30 |
| MLP | 1min | 1:30 |
| LSTM | 1min | 2:30 |
| CNN | 1min30s | 4:00 |
| Conclusion | 1min | 5:00 |

---

## üé§ Script de Transition entre Sections

**Intro ‚Üí MLP :**
> "Commen√ßons par le mod√®le le plus simple : le MLP."

**MLP ‚Üí LSTM :**
> "En passant aux LSTM, nous exploitons l'historique des coups."

**LSTM ‚Üí CNN :**
> "Les CNN permettent de capturer la structure spatiale du plateau."

**CNN ‚Üí Conclusion :**
> "Comparons maintenant tous ces mod√®les."

---

## üìå Questions Anticip√©es

**Q1 : Pourquoi le CNN a-t-il autant d'overfitting ?**
> Dataset trop petit (nombre de parties limit√©) pour la complexit√© du mod√®le. Solution : augmentation des donn√©es en cours.

**Q2 : Quel mod√®le utiliseriez-vous en production ?**
> Le MLP [512, 256] pour sa stabilit√©, ou le CNN avec augmentation des donn√©es pour les performances.

**Q3 : Temps d'entra√Ænement total ?**
> Plusieurs jours de calcul avec grid search exhaustif sur GPU RTX 5070 Ti.

**Q4 : Pourquoi Hidden State LSTM > Output Sequence ?**
> Pour Othello, seul le coup final compte, pas toute la s√©quence de coups possibles.
