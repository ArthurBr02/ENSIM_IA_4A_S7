{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_sa5061tE5q",
        "outputId": "ff322f74-f7ab-477c-c927-c8738a2fcd4b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SHtFHBJLoG8q"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6TYZlG2oG8u"
      },
      "source": [
        "# Load IMDB Dataset of 50K Movie Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5CJ5oJNoG8v"
      },
      "source": [
        "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
        "\n",
        "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. It provides a set of 50,000 highly polar movie reviews.\n",
        "\n",
        "Download data from : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AjSu4MXyoG8w",
        "outputId": "76c6f932-10c0-414e-c5a4-75955e8ee19d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5fdd614b-c419-4211-bb2a-9e8aa48db1d2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5fdd614b-c419-4211-bb2a-9e8aa48db1d2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5fdd614b-c419-4211-bb2a-9e8aa48db1d2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5fdd614b-c419-4211-bb2a-9e8aa48db1d2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7274419b-82fe-4d06-9b2b-a3adec4a7e6d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7274419b-82fe-4d06-9b2b-a3adec4a7e6d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7274419b-82fe-4d06-9b2b-a3adec4a7e6d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# base_csv = 'data/IMDB Dataset.csv'\n",
        "base_csv = '/content/drive/MyDrive/ENSIM_S7/IA/IMDB Dataset.csv'\n",
        "df = pd.read_csv(base_csv)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUU5m4O4oG8y"
      },
      "source": [
        "## Splitting to train and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SPSzbQOoG8z"
      },
      "source": [
        "We select only 10000 reviews (samples) to be able to work in real time woth our calculation capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1NIiAazHoG8z",
        "outputId": "e7c92bbb-9ad2-4b6b-9616-7cfabcb07e52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (7500,)\n",
            "shape of test data is (2500,)\n"
          ]
        }
      ],
      "source": [
        "X,y = df['review'].values,df['sentiment'].values\n",
        "x_train,x_test,y_train,y_test = train_test_split(X[:10000],y[:10000])\n",
        "print(f'shape of train data is {x_train.shape}')\n",
        "print(f'shape of test data is {x_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aMOp0BWroG80",
        "outputId": "dabee810-5a1e-4488-b6c1-ccd7d9d6b59a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the Negative and Positive class\n"
          ]
        }
      ],
      "source": [
        "dd = pd.Series(y_train).value_counts()\n",
        "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
        "plt.show()\n",
        "print(\"Number of samples in the Negative and Positive class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkH5tTiQoG81"
      },
      "source": [
        "## Preprocessing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fo9mxjBoG81"
      },
      "source": [
        "### Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "engqmYK3oG82"
      },
      "source": [
        "Word Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics.\n",
        "\n",
        "**The process of converting words into numbers are called Vectorization.**\n",
        "\n",
        "**One Hot Encoding:**\n",
        "If you have been wo tabular data, one hot encoding must not be a big deal for you! One hot encoding is simply a binary representation of the words in the vocabulary. If there are 25 words in the vocabulary, we will have a vector of length 25, with '1' at the position index of the word, and '0s' at all the other position. A simple example representation is given below, where 5 words are there on the vocabularyÂ¶\n",
        "\n",
        "![image.png](attachment:fd26f729-0365-4fe0-96d4-bcbb0512ec70.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKlTLmUYoG83"
      },
      "source": [
        "In order to vectorize reviews in the dataset, we propose to follow below steps:\n",
        "\n",
        "1. Remove all non_word characters (everything except numbers and letters)\n",
        "2. Create a list of all words in dataset\n",
        "3. Count number of each word and make a sorted list for a pair of (word, frequency)\n",
        "4. Selecting 500 most frequent words in dataset as dictionary with accompany of their one-hot vector. It means all 500 frequent words would be represented by a vector with size of 500 which contain one time 1 and the rest is 0.\n",
        "5. Remove the rest of words that are not in the dictionary\n",
        "6. convert all reviews to a sequence of one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8P9Kqgw0oG83"
      },
      "outputs": [],
      "source": [
        "# 1. Remove all non-word characters (everything except numbers and letters)\n",
        "def preprocess_string(s):\n",
        "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    # Replace all runs of whitespaces with no space\n",
        "    s = re.sub(r\"\\s+\", '', s)\n",
        "    # replace digits with no space\n",
        "    s = re.sub(r\"\\d\", '', s)\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "81Yyd6ohoG84",
        "outputId": "470b353b-c540-4660-b564-e3b358a73b15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "##################\n",
        "#2. Create a list of all words in dataset\n",
        "##################\n",
        "word_list = []\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for sent in x_train:\n",
        "    for word in sent.lower().split():\n",
        "        word = preprocess_string(word)\n",
        "        ##################\n",
        "        #1. Remove all non-word characters\n",
        "        ##################\n",
        "        if word not in stop_words and word != '':\n",
        "            word_list.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KgFIAlHjoG85",
        "outputId": "9c312bde-f170-413c-c284-8295c7573d00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocabulary is 500\n"
          ]
        }
      ],
      "source": [
        "##################\n",
        "#3. Count number of each word and make a sorted list for a pair of (word, frequency)\n",
        "##################\n",
        "corpus = Counter(word_list)\n",
        "##################\n",
        "#4. Selecting 1000 most frequent words in dataset as dictionary with accompany of their one-hot vector.\n",
        "#It means all 1000 frequent words would be represented by a vector with size of 1000 which contain one time 1 and the rest is 0.\n",
        "##################\n",
        "NUM_DICT=500\n",
        "# sorting on the basis of most common words\n",
        "corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:NUM_DICT]\n",
        "# creating a dict\n",
        "onehot_dict={}\n",
        "for i,w in enumerate(corpus_):\n",
        "    one_hot=np.zeros(NUM_DICT)\n",
        "    one_hot[i]=1\n",
        "    onehot_dict[w]=one_hot\n",
        "\n",
        "print(f'Length of vocabulary is {len(onehot_dict)}')\n",
        "\n",
        "def vectorization(x,y,onehot_dict):\n",
        "\n",
        "    ##################\n",
        "    # 5. Remove the rest of words that are not in the dictionary\n",
        "    ##################\n",
        "    encoded_x=[onehot_dict[preprocess_string(word)] for word in x.lower().split()\n",
        "                             if preprocess_string(word) in onehot_dict.keys()]\n",
        "\n",
        "    # convert the positive class to 1 and negative class to 0\n",
        "    encoded_y = 1 if y =='positive' else 0\n",
        "    return np.array(encoded_x), np.array(encoded_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxnvZxj5oG85"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6d1AB27oG85"
      },
      "source": [
        "Now we will pad each of the sequence to max length (we chose 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "k_9SN4lsoG85"
      },
      "outputs": [],
      "source": [
        "def padding_(sentence, seq_len,dic_size=NUM_DICT):\n",
        "    features = np.zeros((seq_len,dic_size),dtype=int)\n",
        "    features[-min(seq_len,len(sentence)):] = np.array(sentence)[:seq_len]\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R23vxjtoG86"
      },
      "source": [
        "### Create a tensor dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xzyG0mo5oG86"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "class CustomDataset():\n",
        "    def __init__(self,x,y,onehot_dict, seq_len=500,dict_size=NUM_DICT):\n",
        "        self.y = y\n",
        "        self.x = x\n",
        "        self.max_len = seq_len\n",
        "        self.dict_size = dict_size\n",
        "        self.onehot_dict=onehot_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def padding_(self,sentence):\n",
        "        features = np.zeros((self.max_len,self.dict_size),dtype=int)\n",
        "        features[-min(self.max_len,len(sentence)):] = np.array(sentence)[:self.max_len]\n",
        "        return features\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_vec,y_vec=vectorization(self.x[idx],self.y[idx],self.onehot_dict)\n",
        "        x_vec_pad=self.padding_(x_vec)\n",
        "        return torch.from_numpy(x_vec_pad), torch.from_numpy(y_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KNxJzd36oG86"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=50\n",
        "# create Tensor datasets\n",
        "train_data = CustomDataset(x_train, y_train,onehot_dict, seq_len=MAX_LEN,dict_size=NUM_DICT)\n",
        "valid_data = CustomDataset(x_test, y_test,onehot_dict, seq_len=MAX_LEN,dict_size=NUM_DICT)\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 100\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HLOt9_n0oG86",
        "outputId": "cdb15540-d690-4872-d428-37ba89187567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (28,500) into shape (28,1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-178692374.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# obtain one batch of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sample input size: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch_size, seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3366008175.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx_vec_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3366008175.py\u001b[0m in \u001b[0;36mpadding_\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpadding_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (28,500) into shape (28,1)"
          ]
        }
      ],
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample output size:', sample_y.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VVkulX5oG87"
      },
      "source": [
        "# RNN Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NjRt9IQroG87",
        "outputId": "53b65d6e-ae30-4024-b549-affe9f1f66e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SA-cTk-IoG87"
      },
      "outputs": [],
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self,no_layers,hidden_dim,embedding_dim,dropout_prob=0.3):\n",
        "        super(SentimentRNN,self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                           num_layers=no_layers, batch_first=True)\n",
        "\n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "        batch_size = x.size(0)\n",
        "        # embeddings and lstm_out\n",
        "        lstm_out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(lstm_out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "khxyJJHqoG88"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# function to predict accuracy\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()/len(label.squeeze())\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, opt,clip):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.train()\n",
        "    # initialize hidden state\n",
        "    batch_size=int(size/num_batches)\n",
        "    h = model.init_hidden(batch_size)\n",
        "    for batch, (inputs, labels) in enumerate(dataloader):\n",
        "\n",
        "        inputs, labels = inputs.float().to(device), labels.to(device)\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        model.zero_grad()\n",
        "        output,h = model(inputs,h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            loss, current = loss.item(), batch * len(inputs)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return model,opt\n",
        "\n",
        "def test(test_dataloader, model, loss_fn):\n",
        "    size = len(test_dataloader.dataset)\n",
        "    num_batches = len(test_dataloader)\n",
        "    batch_size=int(size/num_batches)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_h = model.init_hidden(batch_size)\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs, labels = inputs.float().to(device), labels.to(device)\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in val_h])\n",
        "\n",
        "            model.zero_grad()\n",
        "            output,h = model(inputs,h)\n",
        "#             import pdb\n",
        "#             pdb.set_trace()\n",
        "            val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            accuracy = acc(output,labels)\n",
        "            val_acc += accuracy\n",
        "\n",
        "    print(f\"Test Accuracy: {100*(val_acc/num_batches):>0.1f}%, Avg loss: {np.mean(val_losses):>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Qvvocn2ioG88",
        "outputId": "f075e2a9-364e-4cb7-f1e4-27ad2ad66500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentRNN(\n",
            "  (lstm): LSTM(500, 256, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n",
            "Number of Parameters:1302785\n"
          ]
        }
      ],
      "source": [
        "no_layers = 2\n",
        "vocab_size = 1#len(vocab) + 1 #extra 1 for padding\n",
        "embedding_dim = NUM_DICT\n",
        "output_dim = 1\n",
        "hidden_dim = 256\n",
        "\n",
        "model_1 = SentimentRNN(no_layers,hidden_dim,embedding_dim)\n",
        "\n",
        "#moving to gpu\n",
        "model_1.to(device)\n",
        "print(model_1)\n",
        "total_params = sum(p.numel() for p in model_1.parameters())\n",
        "print(f\"Number of Parameters:{total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4MxrmbXloG89",
        "outputId": "38abf02d-3cfe-49fa-aff0-3ac063e4f6cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.693273  [    0/ 7500]\n",
            "loss: 0.693335  [ 1000/ 7500]\n",
            "loss: 0.694470  [ 2000/ 7500]\n",
            "loss: 0.676336  [ 3000/ 7500]\n",
            "loss: 0.688712  [ 4000/ 7500]\n",
            "loss: 0.692481  [ 5000/ 7500]\n",
            "loss: 0.694455  [ 6000/ 7500]\n",
            "loss: 0.683817  [ 7000/ 7500]\n",
            "Test Accuracy: 50.8%, Avg loss: 0.660268 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.693145  [    0/ 7500]\n",
            "loss: 0.664915  [ 1000/ 7500]\n",
            "loss: 0.645196  [ 2000/ 7500]\n",
            "loss: 0.630729  [ 3000/ 7500]\n",
            "loss: 0.643618  [ 4000/ 7500]\n",
            "loss: 0.552344  [ 5000/ 7500]\n",
            "loss: 0.553628  [ 6000/ 7500]\n",
            "loss: 0.557505  [ 7000/ 7500]\n",
            "Test Accuracy: 76.4%, Avg loss: 0.487423 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.426035  [    0/ 7500]\n",
            "loss: 0.447453  [ 1000/ 7500]\n",
            "loss: 0.501267  [ 2000/ 7500]\n",
            "loss: 0.446369  [ 3000/ 7500]\n",
            "loss: 0.415589  [ 4000/ 7500]\n",
            "loss: 0.525492  [ 5000/ 7500]\n",
            "loss: 0.426099  [ 6000/ 7500]\n",
            "loss: 0.431671  [ 7000/ 7500]\n",
            "Test Accuracy: 78.5%, Avg loss: 0.456390 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.449092  [    0/ 7500]\n",
            "loss: 0.310227  [ 1000/ 7500]\n",
            "loss: 0.463412  [ 2000/ 7500]\n",
            "loss: 0.430071  [ 3000/ 7500]\n",
            "loss: 0.443872  [ 4000/ 7500]\n",
            "loss: 0.455613  [ 5000/ 7500]\n",
            "loss: 0.485737  [ 6000/ 7500]\n",
            "loss: 0.373814  [ 7000/ 7500]\n",
            "Test Accuracy: 79.6%, Avg loss: 0.442004 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.394110  [    0/ 7500]\n",
            "loss: 0.378038  [ 1000/ 7500]\n",
            "loss: 0.388426  [ 2000/ 7500]\n",
            "loss: 0.398550  [ 3000/ 7500]\n",
            "loss: 0.468186  [ 4000/ 7500]\n",
            "loss: 0.384761  [ 5000/ 7500]\n",
            "loss: 0.539081  [ 6000/ 7500]\n",
            "loss: 0.353376  [ 7000/ 7500]\n",
            "Test Accuracy: 79.4%, Avg loss: 0.480272 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model_1.parameters(), lr=lr)\n",
        "\n",
        "clip = 5\n",
        "epochs = 5\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    model_1,optimizer = train(train_loader, model_1, criterion, optimizer,clip)\n",
        "    test(valid_loader, model_1, criterion)\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHTeiKdoG89"
      },
      "source": [
        "<div style=\"border: solid 3px #000;\">\n",
        "    <h1 style=\"text-align: center; color:#000; font-family:Georgia; font-size:26px;\">Exercise :</h1>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 1. Change the one hot vector in the preprocessing to a number. It means instead of converting each word to a vector, each word should be represented by a unique integer value. </h1>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 2. Consequently, change the CustomDataset and its padding function to one 1D sequential data.  </h1>\n",
        "    <p style='text-align: left;'> </p>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 3. Add an embedding layer at the beginning of the network with embedding size of 64. nn.Embedding(vocab_size, embedding_size)  </h1>\n",
        "    <p style='text-align: left;'> </p>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 4. Add a dropout layer before the last fully connected layer. nn.Dropout(0.3)</h1>\n",
        "    <p style='text-align: left;'> </p>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 5. Change LSTM layer with a GRU layer. The init_hidden function should be changed and return only h0, Why?</h1>\n",
        "    <p style='text-align: left;'> </p>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 6. Compare with previous version the number of parameters, the performance in terms of final test accuracy and the convergence of model.</h1>\n",
        "    <p style='text-align: left;'> </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HENlKkcwoG8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e32763-c245-424f-bb04-16570dc2e8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocabulary is 500\n",
            "{'br': 0, 'movie': 1, 'film': 2, 'one': 3, 'like': 4, 'good': 5, 'would': 6, 'even': 7, 'time': 8, 'really': 9, 'see': 10, 'story': 11, 'well': 12, 'much': 13, 'get': 14, 'great': 15, 'bad': 16, 'people': 17, 'also': 18, 'first': 19, 'dont': 20, 'made': 21, 'way': 22, 'movies': 23, 'make': 24, 'films': 25, 'could': 26, 'characters': 27, 'think': 28, 'never': 29, 'watch': 30, 'many': 31, 'little': 32, 'character': 33, 'seen': 34, 'love': 35, 'best': 36, 'show': 37, 'acting': 38, 'two': 39, 'know': 40, 'plot': 41, 'ever': 42, 'life': 43, 'better': 44, 'still': 45, 'say': 46, 'scene': 47, 'end': 48, 'man': 49, 'go': 50, 'something': 51, 'scenes': 52, 'im': 53, 'real': 54, 'doesnt': 55, 'back': 56, 'watching': 57, 'actors': 58, 'years': 59, 'thing': 60, 'look': 61, 'though': 62, 'didnt': 63, 'going': 64, 'nothing': 65, 'actually': 66, 'funny': 67, 'work': 68, 'makes': 69, 'lot': 70, 'another': 71, 'director': 72, 'every': 73, 'new': 74, 'us': 75, 'old': 76, 'find': 77, 'part': 78, 'things': 79, 'pretty': 80, 'cant': 81, 'thats': 82, 'enough': 83, 'young': 84, 'quite': 85, 'got': 86, 'want': 87, 'cast': 88, 'big': 89, 'seems': 90, 'take': 91, 'around': 92, 'may': 93, 'however': 94, 'fact': 95, 'horror': 96, 'must': 97, 'long': 98, 'ive': 99, 'world': 100, 'almost': 101, 'come': 102, 'give': 103, 'without': 104, 'gets': 105, 'original': 106, 'music': 107, 'always': 108, 'saw': 109, 'isnt': 110, 'right': 111, 'theres': 112, 'interesting': 113, 'script': 114, 'anything': 115, 'thought': 116, 'least': 117, 'comedy': 118, 'done': 119, 'might': 120, 'times': 121, 'since': 122, 'point': 123, 'action': 124, 'family': 125, 'whole': 126, 'minutes': 127, 'performance': 128, 'role': 129, 'series': 130, 'feel': 131, 'bit': 132, 'far': 133, 'hes': 134, 'last': 135, 'probably': 136, 'away': 137, 'tv': 138, 'guy': 139, 'kind': 140, 'played': 141, 'anyone': 142, 'yet': 143, 'sure': 144, 'found': 145, 'rather': 146, 'woman': 147, 'worst': 148, 'comes': 149, 'girl': 150, 'course': 151, 'although': 152, 'making': 153, 'shows': 154, 'book': 155, 'believe': 156, 'put': 157, 'hard': 158, 'fun': 159, 'trying': 160, 'especially': 161, 'everything': 162, 'main': 163, 'place': 164, 'dvd': 165, 'maybe': 166, 'screen': 167, 'wasnt': 168, 'day': 169, 'looks': 170, 'three': 171, 'goes': 172, 'american': 173, 'worth': 174, 'idea': 175, 'effects': 176, 'someone': 177, 'money': 178, 'takes': 179, 'ending': 180, 'said': 181, 'sense': 182, 'looking': 183, 'watched': 184, 'set': 185, 'beautiful': 186, 'seem': 187, 'different': 188, 'actor': 189, 'reason': 190, 'excellent': 191, 'plays': 192, 'john': 193, 'true': 194, 'job': 195, 'together': 196, 'night': 197, 'used': 198, 'left': 199, 'war': 200, 'instead': 201, 'special': 202, 'audience': 203, 'everyone': 204, 'play': 205, 'poor': 206, 'nice': 207, 'seeing': 208, 'simply': 209, 'along': 210, 'later': 211, 'house': 212, 'completely': 213, 'star': 214, 'shot': 215, 'kids': 216, 'else': 217, 'read': 218, 'year': 219, 'need': 220, 'home': 221, 'men': 222, 'death': 223, 'black': 224, 'high': 225, 'fan': 226, 'given': 227, 'friends': 228, 'stupid': 229, 'second': 230, 'version': 231, 'use': 232, 'less': 233, 'mind': 234, 'wife': 235, 'father': 236, 'line': 237, 'try': 238, 'youre': 239, 'dead': 240, 'performances': 241, 'help': 242, 'half': 243, 'came': 244, 'wonderful': 245, 'couple': 246, 'classic': 247, 'tell': 248, 'recommend': 249, 'rest': 250, 'production': 251, 'absolutely': 252, 'playing': 253, 'hollywood': 254, 'truly': 255, 'wrong': 256, 'short': 257, 'full': 258, 'next': 259, 'keep': 260, 'boring': 261, 'let': 262, 'remember': 263, 'others': 264, 'gives': 265, 'finally': 266, 'getting': 267, 'camera': 268, 'human': 269, 'terrible': 270, 'small': 271, 'enjoy': 272, 'either': 273, 'person': 274, 'often': 275, 'perhaps': 276, 'name': 277, 'understand': 278, 'face': 279, 'game': 280, 'episode': 281, 'piece': 282, 'women': 283, 'mother': 284, 'moments': 285, 'mean': 286, 'awful': 287, 'guys': 288, 'video': 289, 'perfect': 290, 'definitely': 291, 'stars': 292, 'couldnt': 293, 'become': 294, 'case': 295, 'totally': 296, 'budget': 297, 'head': 298, 'dialogue': 299, 'start': 300, 'went': 301, 'children': 302, 'worse': 303, 'certainly': 304, 'school': 305, 'youll': 306, 'several': 307, 'early': 308, 'loved': 309, 'lines': 310, 'top': 311, 'yes': 312, 'beginning': 313, 'based': 314, 'liked': 315, 'title': 316, 'lives': 317, 'sort': 318, 'throughout': 319, 'picture': 320, 'style': 321, 'waste': 322, 'final': 323, 'sound': 324, 'sex': 325, 'direction': 326, 'cinema': 327, 'entire': 328, 'oh': 329, 'felt': 330, 'despite': 331, 'supposed': 332, 'live': 333, 'evil': 334, 'id': 335, 'written': 336, 'fine': 337, 'fans': 338, 'lost': 339, 'laugh': 340, 'able': 341, 'shes': 342, 'seemed': 343, 'problem': 344, 'overall': 345, 'entertaining': 346, 'friend': 347, 'drama': 348, 'low': 349, 'days': 350, 'example': 351, 'wants': 352, 'hope': 353, 'mr': 354, 'gave': 355, 'lead': 356, 'stuff': 357, 'killer': 358, 'matter': 359, 'act': 360, 'town': 361, 'turn': 362, 'already': 363, 'girls': 364, 'writing': 365, 'called': 366, 'boy': 367, 'turns': 368, 'wont': 369, 'dark': 370, 'care': 371, 'car': 372, 'history': 373, 'becomes': 374, 'kill': 375, 'white': 376, 'humor': 377, 'extremely': 378, 'theyre': 379, 'guess': 380, 'favorite': 381, 'brilliant': 382, 'looked': 383, 'wanted': 384, 'past': 385, 'enjoyed': 386, 'son': 387, 'works': 388, 'tries': 389, 'heart': 390, 'parts': 391, 'ones': 392, 'quality': 393, 'unfortunately': 394, 'michael': 395, 'soon': 396, 'lack': 397, 'late': 398, 'sometimes': 399, 'eyes': 400, 'city': 401, 'amazing': 402, 'behind': 403, 'horrible': 404, 'close': 405, 'starts': 406, 'actress': 407, 'took': 408, 'side': 409, 'highly': 410, 'fight': 411, 'decent': 412, 'run': 413, 'obviously': 414, 'viewer': 415, 'child': 416, 'ill': 417, 'except': 418, 'flick': 419, 'feeling': 420, 'known': 421, 'directed': 422, 'killed': 423, 'murder': 424, 'told': 425, 'particularly': 426, 'hour': 427, 'coming': 428, 'moment': 429, 'art': 430, 'leave': 431, 'expect': 432, 'chance': 433, 'robert': 434, 'stop': 435, 'says': 436, 'attempt': 437, 'god': 438, 'itbr': 439, 'wouldnt': 440, 'thinking': 441, 'english': 442, 'hell': 443, 'group': 444, 'number': 445, 'order': 446, 'genre': 447, 'reality': 448, 'saying': 449, 'kid': 450, 'obvious': 451, 'whose': 452, 'experience': 453, 'involved': 454, 'save': 455, 'score': 456, 'age': 457, 'slow': 458, 'simple': 459, 'wonder': 460, 'james': 461, 'heard': 462, 'david': 463, 'taken': 464, 'complete': 465, 'ok': 466, 'ago': 467, 'type': 468, 'huge': 469, 'violence': 470, 'voice': 471, 'serious': 472, 'happen': 473, 'word': 474, 'novel': 475, 'opening': 476, 'important': 477, 'daughter': 478, 'relationship': 479, 'annoying': 480, 'cannot': 481, 'none': 482, 'anyway': 483, 'lets': 484, 'etc': 485, 'happens': 486, 'due': 487, 'across': 488, 'shown': 489, 'exactly': 490, 'today': 491, 'ridiculous': 492, 'view': 493, 'seriously': 494, 'body': 495, 'hours': 496, 'possible': 497, 'living': 498, 'shots': 499}\n"
          ]
        }
      ],
      "source": [
        "nb_dict = {}\n",
        "for i,w in enumerate(corpus_):\n",
        "    nb_dict[w]=i\n",
        "\n",
        "print(f'Length of vocabulary is {len(nb_dict)}')\n",
        "print(nb_dict)\n",
        "\n",
        "def vectorization(x,y,nb_dict):\n",
        "\n",
        "    ##################\n",
        "    # 5. Remove the rest of words that are not in the dictionary\n",
        "    ##################\n",
        "    encoded_x=[nb_dict[preprocess_string(word)] for word in x.lower().split()\n",
        "                             if preprocess_string(word) in onehot_dict.keys()]\n",
        "\n",
        "    # convert the positive class to 1 and negative class to 0\n",
        "    encoded_y = 1 if y =='positive' else 0\n",
        "    return np.array(encoded_x), np.array(encoded_y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset():\n",
        "    def __init__(self,x,y,nb_dict, seq_len=500,dict_size=NUM_DICT):\n",
        "        self.y = y\n",
        "        self.x = x\n",
        "        self.max_len = seq_len\n",
        "        self.dict_size = dict_size\n",
        "        self.nb_dict=nb_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def padding_(self,sentence):\n",
        "        features = np.zeros((self.max_len,1),dtype=int)\n",
        "        features[-min(self.max_len,len(sentence)):] = np.array(sentence)[:self.max_len]\n",
        "        return features\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_vec,y_vec=vectorization(self.x[idx],self.y[idx],self.nb_dict)\n",
        "        x_vec_pad=self.padding_(x_vec)\n",
        "        return torch.from_numpy(x_vec_pad), torch.from_numpy(y_vec)"
      ],
      "metadata": {
        "id": "H7c_GJTvwREE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3lZpUqP9oG8-"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=50\n",
        "# create Tensor datasets\n",
        "train_data = CustomDataset(x_train, y_train,onehot_dict, seq_len=MAX_LEN,dict_size=NUM_DICT)\n",
        "valid_data = CustomDataset(x_test, y_test,onehot_dict, seq_len=MAX_LEN,dict_size=NUM_DICT)\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 100\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample output size:', sample_y.size())"
      ],
      "metadata": {
        "id": "_cRgrJ-2xkM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_layers = 2\n",
        "vocab_size = 1#len(vocab) + 1 #extra 1 for padding\n",
        "embedding_dim = NUM_DICT\n",
        "output_dim = 1\n",
        "hidden_dim = 256\n",
        "\n",
        "model_1 = SentimentRNN(no_layers,hidden_dim,embedding_dim)\n",
        "\n",
        "#moving to gpu\n",
        "model_1.to(device)\n",
        "print(model_1)\n",
        "total_params = sum(p.numel() for p in model_1.parameters())\n",
        "print(f\"Number of Parameters:{total_params}\")"
      ],
      "metadata": {
        "id": "3blysRdxxlGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model_1.parameters(), lr=lr)\n",
        "\n",
        "clip = 5\n",
        "epochs = 5\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    model_1,optimizer = train(train_loader, model_1, criterion, optimizer,clip)\n",
        "    test(valid_loader, model_1, criterion)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "2ckQhgMixtYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39jt8aQ5oG8_"
      },
      "source": [
        "<div style=\"border: solid 3px #000;\">\n",
        "    <h1 style=\"text-align: center; color:#000; font-family:Georgia; font-size:26px;\">Exercise :</h1>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 1. We would like to add a convolutional layer after embedding layer and before LSTM layer. Why we should use 1D convolutional layer instead of 2D?</h1>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 2. Conv1d would be applied to the last axis of data. In order to apply the convolution operation on the axis of words order we can change the shape of data by using x.permute(0, 2, 1); the command would change the shape of x by exchanging the second and third dimension.  </h1>\n",
        "    <p style='text-align: left;'> </p>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 3. Add a MaxPool1d after Conv1d layer </h1>\n",
        "    <p style='text-align: left;'> </p>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 4. By applying x.permute(0, 2, 1) once again on the output of the MaxPool1d layer, change the dimensions order to the original form. </h1>\n",
        "    <p style='text-align: left;'> </p>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 5. Compare with previous version the number of parameters, training time (what has changed?) the performance in terms of final test accuracy and the convergence of model.</h1>\n",
        "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 6. Check if the model has been overfitted.</h1>\n",
        "    <p style='text-align: left;'> </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsVkIZMroG8_"
      },
      "outputs": [],
      "source": [
        "# Your code is missing here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo0yFj8ioG9A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}