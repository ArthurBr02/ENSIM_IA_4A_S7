{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your CNN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start by creating a class that inherits the nn.Module class, and then we define the layers and their sequence of execution inside \\_\\_init\\_\\_ and forward respectively.\n",
    "\n",
    "Some things to notice here:\n",
    "\n",
    "- nn.Conv2d is used to define the convolutional layers. We define the channels they receive and how much should they return along with the kernel size. We start from 1 channels, as we are using Grayscale images\n",
    "- nn.MaxPool2d is a max-pooling layer that just requires the kernel size and the stride\n",
    "- nn.Linear is the fully connected layer, and nn.ReLU is the activation function used\n",
    "\n",
    "In the forward method, we define the sequence, and, before the fully connected layers, we reshape the output to match the input to a fully connected layer\n",
    "\n",
    "Source: https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "\n",
      "ConvNeuralNet(\n",
      "  (conv_layer1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_layer2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu1): ReLU()\n",
      "  (fc1): Linear(in_features=400, out_features=10, bias=True)\n",
      ")\n",
      "Number of Parameters:5258\n"
     ]
    }
   ],
   "source": [
    "# Creating a CNN class\n",
    "class ConvNeuralNet(torch.nn.Module):\n",
    "    #  Determine what layers and their order in CNN object \n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.conv_layer1 = torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "        self.max_pool1 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.conv_layer2 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.max_pool2 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.fc1 = torch.nn.Linear(400, num_classes)\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.max_pool2(out)\n",
    "                \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.relu1(out)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\\n\")\n",
    "\n",
    "model = ConvNeuralNet(10).to(device)\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of Parameters:{total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.309520  [    0/60000]\n",
      "loss: 2.205447  [ 6400/60000]\n",
      "loss: 1.814829  [12800/60000]\n",
      "loss: 1.026267  [19200/60000]\n",
      "loss: 0.957518  [25600/60000]\n",
      "loss: 0.793453  [32000/60000]\n",
      "loss: 0.603216  [38400/60000]\n",
      "loss: 0.875839  [44800/60000]\n",
      "loss: 0.726147  [51200/60000]\n",
      "loss: 0.822019  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.731314 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.825124  [    0/60000]\n",
      "loss: 0.682887  [ 6400/60000]\n",
      "loss: 0.727694  [12800/60000]\n",
      "loss: 0.578035  [19200/60000]\n",
      "loss: 0.733218  [25600/60000]\n",
      "loss: 0.922614  [32000/60000]\n",
      "loss: 0.606198  [38400/60000]\n",
      "loss: 0.700576  [44800/60000]\n",
      "loss: 0.851536  [51200/60000]\n",
      "loss: 0.596363  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.625841 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.472479  [    0/60000]\n",
      "loss: 0.424525  [ 6400/60000]\n",
      "loss: 0.330385  [12800/60000]\n",
      "loss: 0.420370  [19200/60000]\n",
      "loss: 0.818670  [25600/60000]\n",
      "loss: 0.576596  [32000/60000]\n",
      "loss: 0.428992  [38400/60000]\n",
      "loss: 0.577265  [44800/60000]\n",
      "loss: 0.538265  [51200/60000]\n",
      "loss: 0.474315  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.594684 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.575229  [    0/60000]\n",
      "loss: 0.645013  [ 6400/60000]\n",
      "loss: 0.381956  [12800/60000]\n",
      "loss: 0.436454  [19200/60000]\n",
      "loss: 0.480298  [25600/60000]\n",
      "loss: 0.369039  [32000/60000]\n",
      "loss: 0.408270  [38400/60000]\n",
      "loss: 0.464291  [44800/60000]\n",
      "loss: 0.564042  [51200/60000]\n",
      "loss: 0.498510  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.529617 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.564089  [    0/60000]\n",
      "loss: 0.608638  [ 6400/60000]\n",
      "loss: 0.338361  [12800/60000]\n",
      "loss: 0.522333  [19200/60000]\n",
      "loss: 0.372090  [25600/60000]\n",
      "loss: 0.594895  [32000/60000]\n",
      "loss: 0.619064  [38400/60000]\n",
      "loss: 0.461264  [44800/60000]\n",
      "loss: 0.418994  [51200/60000]\n",
      "loss: 0.548353  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.548077 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.604515  [    0/60000]\n",
      "loss: 0.581087  [ 6400/60000]\n",
      "loss: 0.382801  [12800/60000]\n",
      "loss: 0.508159  [19200/60000]\n",
      "loss: 0.438140  [25600/60000]\n",
      "loss: 0.458481  [32000/60000]\n",
      "loss: 0.419345  [38400/60000]\n",
      "loss: 0.457756  [44800/60000]\n",
      "loss: 0.332040  [51200/60000]\n",
      "loss: 0.398427  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.480514 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.337724  [    0/60000]\n",
      "loss: 0.390794  [ 6400/60000]\n",
      "loss: 0.327256  [12800/60000]\n",
      "loss: 0.506069  [19200/60000]\n",
      "loss: 0.461295  [25600/60000]\n",
      "loss: 0.439148  [32000/60000]\n",
      "loss: 0.412645  [38400/60000]\n",
      "loss: 0.382177  [44800/60000]\n",
      "loss: 0.336631  [51200/60000]\n",
      "loss: 0.364545  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.461146 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.502396  [    0/60000]\n",
      "loss: 0.423720  [ 6400/60000]\n",
      "loss: 0.407271  [12800/60000]\n",
      "loss: 0.476560  [19200/60000]\n",
      "loss: 0.382996  [25600/60000]\n",
      "loss: 0.428572  [32000/60000]\n",
      "loss: 0.414860  [38400/60000]\n",
      "loss: 0.314951  [44800/60000]\n",
      "loss: 0.556652  [51200/60000]\n",
      "loss: 0.483213  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.451697 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.346577  [    0/60000]\n",
      "loss: 0.437089  [ 6400/60000]\n",
      "loss: 0.279997  [12800/60000]\n",
      "loss: 0.348986  [19200/60000]\n",
      "loss: 0.522534  [25600/60000]\n",
      "loss: 0.331745  [32000/60000]\n",
      "loss: 0.388727  [38400/60000]\n",
      "loss: 0.346860  [44800/60000]\n",
      "loss: 0.489105  [51200/60000]\n",
      "loss: 0.565282  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.473112 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.459218  [    0/60000]\n",
      "loss: 0.435601  [ 6400/60000]\n",
      "loss: 0.577503  [12800/60000]\n",
      "loss: 0.413368  [19200/60000]\n",
      "loss: 0.782717  [25600/60000]\n",
      "loss: 0.363247  [32000/60000]\n",
      "loss: 0.327905  [38400/60000]\n",
      "loss: 0.355524  [44800/60000]\n",
      "loss: 0.289764  [51200/60000]\n",
      "loss: 0.382782  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.437408 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define relevant variables for the ML task\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Set Loss function with criterion\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: solid 3px #000;\">\n",
    "    <h1 style=\"text-align: center; color:#000; font-family:Georgia; font-size:26px;\">Exercise :</h1>\n",
    "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; Change the out_channels of the first conv2d layer to 4 and the out_channels of the second conv2d layer to 8. It is necessory to adapt the rest of layer to this new size. Check that if the training can be done. </h1>\n",
    "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 1. By this modification, how much the number of parameters would be reduced? </h1>\n",
    "    <p style='text-align: left;'> </p>\n",
    "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 2. How would this modification has impact on training and convergence? </h1>\n",
    "    <p style='text-align: left;'> </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code is missing here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: solid 3px #000;\">\n",
    "    <h1 style=\"text-align: center; color:#000; font-family:Georgia; font-size:26px;\">Exercise :</h1>\n",
    "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; Remove the first MaxPool2d layer and adapt the size of the rest of layers to be trainable. </h1>\n",
    "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 1. By this modification, how the number of parameters would change? Does it mean that MAxPool layer contains trainable parameters?</h1>\n",
    "    <p style='text-align: left;'> </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code is missing here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: solid 3px #000;\">\n",
    "    <h1 style=\"text-align: center; color:#000; font-family:Georgia; font-size:26px;\">Exercise :</h1>\n",
    "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; By using a print of the out.shape() in the function of forward in Network class, the size of each layer's output can be diplayed (ONLY in the training mode.) </h1>\n",
    "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 1. Add another Conv2d with stride=2 and MaxPool2d with stride=2 beween the first MaxPool2d and the second Conv2d layer.</h1>\n",
    "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 2. Modify the kenel size to 2 in Conv2d layers. Adapt the rest of layers size to have a trainable network.</h1>\n",
    "    <h1 style=\"text-align: left; color:#000; font-family:Courier; font-size:16px;\"> &emsp; 3. While another Conv2d layer is added, how is the number of model's parameters changed?.</h1>\n",
    "    <p style='text-align: left;'> </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code is missing here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
